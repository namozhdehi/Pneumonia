{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpNG3C8DwE/S5BZqKWQ7KY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namozhdehi/Pneumonia/blob/main/05_modeling_Pneumonia_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Modeling<a id='5_Modeling'></a>"
      ],
      "metadata": {
        "id": "avZaFHzaB3Qo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Contents<a id='5.1_Contents'></a>\n",
        "* [5 Modeling](#5_Modeling)\n",
        "  * [5.1 Contents](#5.1_Contents)\n",
        "  * [5.2 Introduction](#5.2_Introduction)\n",
        "  * [5.3 Imports](#5.3_Imports)\n",
        "  * [5.4 Loading Data](#5.4_Loading_Data)\n",
        "  * [5.5 Model Architecture](#5.5_Model_Architecture)\n",
        "  * [5.6 Training the Model](#5.6_Training)\n",
        "  * [5.7 Validation and Testing](#5.7_Validation_Testing)\n",
        "  * [5.8 Save the Model](#5.8_Save_Model)\n",
        "  * [5.9 Summary](#5.9_Summary)"
      ],
      "metadata": {
        "id": "aZF5JyaUr3EN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Introduction<a id='5.2_Introduction'></a>"
      ],
      "metadata": {
        "id": "ZUyYr21Cr4Sz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook focuses on improving and training a deep learning model using TensorFlow and Keras for pneumonia detection based on chest X-ray images. We use a pre-trained EfficientNet-B7 model for binary classification (Pneumonia vs Normal). The dataset is preprocessed and augmented with advanced techniques, and the model's performance is evaluated using accuracy, loss, and other evaluation metrics."
      ],
      "metadata": {
        "id": "YTO1xtFIr7Er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Imports<a id='5.3_Imports'></a>"
      ],
      "metadata": {
        "id": "lHoiTrxqr92O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB7\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "7p_-66iYsBNc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enable Mixed Precision and XLA Compilation**"
      ],
      "metadata": {
        "id": "uhsEFlzjZG3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable XLA and Mixed Precision\n",
        "from tensorflow.keras import mixed_precision\n",
        "tf.config.optimizer.set_jit(True)  # Enable XLA for speed\n",
        "\n",
        "# Enable mixed precision training\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "print(f\"Compute dtype: {policy.compute_dtype}\")\n",
        "print(f\"Variable dtype: {policy.variable_dtype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5wEz1PfZEFY",
        "outputId": "d378c98a-a1ed-487b-fc07-1d3a283fd8b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compute dtype: float16\n",
            "Variable dtype: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 Loading Data<a id='5.4_Loading_Data'></a>"
      ],
      "metadata": {
        "id": "jbM_vRp6sHLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image size and batch size\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 128  # Increase batch size for faster processing\n",
        "\n",
        "# Directories for training, validation, and test data\n",
        "train_dir = 'Data/chest_xray/train'\n",
        "val_dir = 'Data/chest_xray/val'\n",
        "test_dir = 'Data/chest_xray/test'\n",
        "\n",
        "# Use tf.data.Dataset instead of ImageDataGenerator for performance\n",
        "def process_image(image, label):\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = image / 255.0  # Normalize\n",
        "    return image, label\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_data(directory, batch_size):\n",
        "    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory,\n",
        "        label_mode=\"binary\",\n",
        "        image_size=IMG_SIZE,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    dataset = dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_ds = load_data(train_dir, BATCH_SIZE)\n",
        "val_ds = load_data(val_dir, BATCH_SIZE)\n",
        "test_ds = load_data(test_dir, BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL_4NeGQtAzL",
        "outputId": "5725ab11-6115-4d7e-bc0c-8d8bc5592700"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 files belonging to 2 classes.\n",
            "Found 16 files belonging to 2 classes.\n",
            "Found 624 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4.1 Setup Kaggle API"
      ],
      "metadata": {
        "id": "jgmVOkzZsM0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets up the Kaggle API credentials to download the \"chest-xray-pneumonia\" dataset from Kaggle, unzips the dataset into a folder named \"Data/chest_xray,\" and checks if the directory exists, raising an error if it doesn't."
      ],
      "metadata": {
        "id": "E5XBKsCrsRyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Set up Kaggle API credentials\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/kaggle.json\"  # Update this path\n",
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "\n",
        "# Unzip the downloaded file\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('chest-xray-pneumonia.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('Data')  # Extract to a folder named 'chest_xray'\n",
        "\n",
        "# Define the data directory where the dataset is extracted\n",
        "data_dir = 'Data/chest_xray'\n",
        "\n",
        "\n",
        "# Check if data_dir exists\n",
        "if not os.path.exists(data_dir):\n",
        "    raise FileNotFoundError(f\"The dataset directory '{data_dir}' does not exist. Please check the path.\")\n",
        "    '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "wpWmfhvpsTjt",
        "outputId": "9c6b9027-f11b-460d-9c5b-455778eff53b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Set up Kaggle API credentials\\nos.environ[\\'KAGGLE_CONFIG_DIR\\'] = \"https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/kaggle.json\"  # Update this path\\n\\n# Download the dataset\\n!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\\n\\n# Unzip the downloaded file\\nimport zipfile\\n\\nwith zipfile.ZipFile(\\'chest-xray-pneumonia.zip\\', \\'r\\') as zip_ref:\\n    zip_ref.extractall(\\'Data\\')  # Extract to a folder named \\'chest_xray\\'\\n\\n# Define the data directory where the dataset is extracted\\ndata_dir = \\'Data/chest_xray\\'\\n\\n\\n# Check if data_dir exists\\nif not os.path.exists(data_dir):\\n    raise FileNotFoundError(f\"The dataset directory \\'{data_dir}\\' does not exist. Please check the path.\")\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 Model Architecture<a id='5.5_Model_Architecture'></a>"
      ],
      "metadata": {
        "id": "rlcvvb0gsWix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-GPU support using MirroredStrategy\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with strategy.scope():\n",
        "    # Load EfficientNetB7 model pre-trained on ImageNet\n",
        "    base_model = EfficientNetB7(include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "\n",
        "    # Freeze base layers to prevent updating during initial training\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Add custom layers for binary classification\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(0.5)(x)  # Reduce dropout for faster convergence\n",
        "    output = Dense(1, activation='sigmoid')(x)  # Sigmoid for binary classification\n",
        "\n",
        "    # Create the full model\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Q-UYcdiMsXy6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.6 Training the Model<a id='5.6_Training'></a>"
      ],
      "metadata": {
        "id": "dR0GMMwzsbhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks for early stopping and learning rate reduction\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=1e-6)\n",
        "\n",
        "with strategy.scope():\n",
        "    # Train the model using mixed precision and XLA optimizations\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=6,  # Reduce epochs for quicker training\n",
        "        callbacks=[early_stop, reduce_lr]\n",
        "    )\n",
        "\n",
        "    # Unfreeze base layers and fine-tune the model\n",
        "    base_model.trainable = True\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-5),  # Lower learning rate for fine-tuning\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Fine-tune the model\n",
        "    history_finetune = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=3,  # Further reduce fine-tuning epochs\n",
        "        callbacks=[early_stop, reduce_lr]\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDvindY9scx-",
        "outputId": "bcdcd0ce-2daf-44ab-fb50-40ffc7db3011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.7 Plot Training History<a id='5.7_History'></a>"
      ],
      "metadata": {
        "id": "d9F4MtS3V_Iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy and loss\n",
        "\n",
        "def plot_history(history, history_finetune):\n",
        "    # Merge history and fine-tune history for plotting\n",
        "    acc = history.history['accuracy'] + history_finetune.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy'] + history_finetune.history['val_accuracy']\n",
        "    loss = history.history['loss'] + history_finetune.history['loss']\n",
        "    val_loss = history.history['val_loss'] + history_finetune.history['val_loss']\n",
        "\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "    plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history, history_finetune)\n"
      ],
      "metadata": {
        "id": "4PxvVDP3V_ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.8 Validation and Testing<a id='5.8_Validation_Testing'></a>"
      ],
      "metadata": {
        "id": "UITXcWX-sgK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test data\n",
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print(f'Test Accuracy: {test_acc}')\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(test_ds)\n",
        "predicted_classes = np.where(predictions > 0.5, 1, 0)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(test_ds.classes, predicted_classes)\n",
        "print('Confusion Matrix')\n",
        "print(cm)\n",
        "\n",
        "# Classification Report\n",
        "report = classification_report(test_ds.classes, predicted_classes, target_names=['Normal', 'Pneumonia'])\n",
        "print('Classification Report')\n",
        "print(report)"
      ],
      "metadata": {
        "id": "CNe-eewvsgsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.8.1 Plot Confusion Matrix<a id='5.7.1_Matrix'></a>"
      ],
      "metadata": {
        "id": "uQZD_w-4ZQfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in np.ndindex(cm.shape):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plot_confusion_matrix(cm, classes=['Normal', 'Pneumonia'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "luf8y_QM-G-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.9 Save the Model<a id='5.9_Save_Model'></a>"
      ],
      "metadata": {
        "id": "vN0wB7_6skmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model.save('pneumonia_detection_model.h5')"
      ],
      "metadata": {
        "id": "mO_6POYpsnaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.10 Summary<a id='5.10_Summary'></a>"
      ],
      "metadata": {
        "id": "nxIco4Dwsr62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we implemented a deep learning model for pneumonia detection using EfficientNet-B7 pre-trained on ImageNet. The model was fine-tuned using techniques such as early stopping, learning rate reduction, and dropout to prevent overfitting.\n",
        "\n",
        "We employed data augmentation on the training data to improve generalization. After training, the model achieved satisfactory results in terms of accuracy. The confusion matrix and classification report provided insights into the model's performance for each class (Normal vs Pneumonia). Further improvements could be achieved through techniques like MixUp augmentation, more extensive fine-tuning, or hyperparameter tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "9YhoF5sbsuVd"
      }
    }
  ]
}