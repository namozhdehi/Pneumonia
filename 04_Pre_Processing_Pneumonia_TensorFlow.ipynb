{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSkpvKeTQrIsRqybcOacDf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namozhdehi/Pneumonia/blob/main/04_Pre_Processing_Pneumonia_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Pre-Processing<a id='4_PreProcessing'></a>"
      ],
      "metadata": {
        "id": "WUeBzcAa0lSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Contents<a id='4.1_Contents'></a>\n",
        "* [4 PreProcessing](#4_PreProcessing)\n",
        "  * [4.1 Contents](#4.1_Contents)\n",
        "  * [4.2 Introduction](#4.2_Introduction)\n",
        "  * [4.3 Imports](#4.3_Imports)\n",
        "  * [4.4 Data Loading and Setup](#4.4_Data_Loading)\n",
        "  * [4.5 Image Transformations](#4.5_Transformations)\n",
        "  * [4.6 Data Augmentation](#4.6_Augmentation)\n",
        "  * [4.7 Train/Validation/Test Splitting](#4.7_Splitting)\n",
        "  * [4.8 DataLoader Creation](#4.8_DataLoader)\n",
        "  * [4.9 Removing Duplicates](#4.9_Remove_Duplicates)\n",
        "  * [4.10 Display Sample Batch](#4.10_Batch)\n",
        "  * [4.11 Save Processed Data](#4.11_Save)"
      ],
      "metadata": {
        "id": "N9hsKTArv-_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Introduction<a id='4.2_Introduction'></a>"
      ],
      "metadata": {
        "id": "GvVQhjT4wBy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will preprocess the chest X-ray dataset for pneumonia detection using TensorFlow and Keras. This involves loading the images, applying image transformations, splitting the dataset into training, validation, and test sets, and finally creating TensorFlow data pipelines to feed the images into the model during training. These preprocessing steps are crucial for training a deep learning model efficiently and ensuring that the data is ready for input."
      ],
      "metadata": {
        "id": "Qlnkbo7lwEjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Imports<a id='4.3_Imports'></a>"
      ],
      "metadata": {
        "id": "6hXVKG0QwHc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image\n",
        "from PIL import UnidentifiedImageError\n"
      ],
      "metadata": {
        "id": "Ds1jX8f1wIzo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Data Loading and Setup<a id='4.4_Data_Loading'></a>"
      ],
      "metadata": {
        "id": "cDc8TAWYwLP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the images are stored\n",
        "data_dir = 'Data/chest_xray'\n",
        "\n",
        "# Load CSV files from the EDA step\n",
        "train_df = pd.read_csv('pneumonia_EDA_train.csv')\n",
        "val_df = pd.read_csv('pneumonia_EDA_val.csv')\n",
        "test_df = pd.read_csv('pneumonia_EDA_test.csv')\n",
        "\n",
        "# Print the size of datasets\n",
        "print(f\"Training Set: {train_df.shape}\")\n",
        "print(f\"Validation Set: {val_df.shape}\")\n",
        "print(f\"Test Set: {test_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRi2nWWrwQaB",
        "outputId": "e224e6ca-b20a-490b-e4d8-cef20ba859c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set: (4172, 3)\n",
            "Validation Set: (1044, 3)\n",
            "Test Set: (624, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4.1 Setup Kaggle API<a id='2.5_Kaggle'></a>"
      ],
      "metadata": {
        "id": "Ku8HpkWPwZmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets up the Kaggle API credentials to download the \"chest-xray-pneumonia\" dataset from Kaggle, unzips the dataset into a folder named \"Data/chest_xray,\" and checks if the directory exists, raising an error if it doesn't."
      ],
      "metadata": {
        "id": "0yAhuAHcwb_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Set up Kaggle API credentials\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/kaggle.json\"  # Update this path\n",
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "\n",
        "# Unzip the downloaded file\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('chest-xray-pneumonia.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('Data')  # Extract to a folder named 'chest_xray'\n",
        "\n",
        "# Define the data directory where the dataset is extracted\n",
        "data_dir = 'Data/chest_xray'\n",
        "\n",
        "\n",
        "# Check if data_dir exists\n",
        "if not os.path.exists(data_dir):\n",
        "    raise FileNotFoundError(f\"The dataset directory '{data_dir}' does not exist. Please check the path.\")\n",
        "    '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "WucC_BgqwZIk",
        "outputId": "32dafa8f-0027-41e0-a4e4-921f93fe3d8b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Set up Kaggle API credentials\\nos.environ[\\'KAGGLE_CONFIG_DIR\\'] = \"https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/kaggle.json\"  # Update this path\\n\\n# Download the dataset\\n!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\\n\\n# Unzip the downloaded file\\nimport zipfile\\n\\nwith zipfile.ZipFile(\\'chest-xray-pneumonia.zip\\', \\'r\\') as zip_ref:\\n    zip_ref.extractall(\\'Data\\')  # Extract to a folder named \\'chest_xray\\'\\n\\n# Define the data directory where the dataset is extracted\\ndata_dir = \\'Data/chest_xray\\'\\n\\n\\n# Check if data_dir exists\\nif not os.path.exists(data_dir):\\n    raise FileNotFoundError(f\"The dataset directory \\'{data_dir}\\' does not exist. Please check the path.\")\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Image Transformations<a id='4.5_Transformations'></a>"
      ],
      "metadata": {
        "id": "n85Wb8ZLwi9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the transformations we will apply to the images, such as resizing, rescaling, and augmentation."
      ],
      "metadata": {
        "id": "7HIfR4RewlWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data rescaling and resizing for all datasets\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "8LbZLhqQwmhX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6 Data Augmentation<a id='4.6_Augmentation'></a>"
      ],
      "metadata": {
        "id": "83Ojlxwwwpjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the training dataset, we will apply data augmentation techniques to artificially increase the size of the dataset and improve the model's ability to generalize."
      ],
      "metadata": {
        "id": "SHhi7AWows-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numerical labels to string format\n",
        "train_df['label'] = train_df['label'].apply(lambda x: 'Normal' if x == 0 else 'Pneumonia')\n",
        "val_df['label'] = val_df['label'].apply(lambda x: 'Normal' if x == 0 else 'Pneumonia')\n",
        "test_df['label'] = test_df['label'].apply(lambda x: 'Normal' if x == 0 else 'Pneumonia')\n",
        "\n",
        "# Data Augmentation for Training Data\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory=data_dir + '/train',\n",
        "    x_col=\"image_path\",\n",
        "    y_col=\"label\",\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dlmc4tYLwuOY",
        "outputId": "d522ca78-c69d-4a53-d311-ec5149f3ee6c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 validated image filenames belonging to 0 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py:920: UserWarning: Found 4172 invalid image filename(s) in x_col=\"image_path\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.7 Train/Validation/Test Splitting<a id='4.7_Splitting'></a>"
      ],
      "metadata": {
        "id": "DcTS8dnhwyZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the data is already divided into train, validation, and test sets, we will create TensorFlow datasets using ImageDataGenerator."
      ],
      "metadata": {
        "id": "BpPxNMkQw0Ae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation and Test Data Loaders\n",
        "val_generator = val_test_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    directory=data_dir + '/val',\n",
        "    x_col=\"image_path\",\n",
        "    y_col=\"label\",\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    directory=data_dir + '/test',\n",
        "    x_col=\"image_path\",\n",
        "    y_col=\"label\",\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFrHc3y9w04Q",
        "outputId": "6c02ec23-ae26-4142-d240-94bc29e0ec5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 validated image filenames belonging to 0 classes.\n",
            "Found 0 validated image filenames belonging to 0 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py:920: UserWarning: Found 1044 invalid image filename(s) in x_col=\"image_path\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py:920: UserWarning: Found 624 invalid image filename(s) in x_col=\"image_path\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.8 DataLoader Creation<a id='4.8_DataLoader'></a>"
      ],
      "metadata": {
        "id": "FSSUhOr5w4VD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create DataLoader objects (i.e., ImageDataGenerator in TensorFlow/Keras) to efficiently load batches of data during training."
      ],
      "metadata": {
        "id": "Y4iFrHhGw5q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the sizes of the datasets\n",
        "print(f\"Training Batches: {len(train_generator)}\")\n",
        "print(f\"Validation Batches: {len(val_generator)}\")\n",
        "print(f\"Test Batches: {len(test_generator)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE2gMAcRw6jb",
        "outputId": "6584901b-d4b6-4be9-9a1d-9ccf481570c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Batches: 0\n",
            "Validation Batches: 0\n",
            "Test Batches: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.9 Removing Duplicates<a id='4.9_Remove_Duplicates'></a>"
      ],
      "metadata": {
        "id": "i4Pbwvb9w-_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since duplicate images can skew model training, letâ€™s remove any duplicate images found based on file content."
      ],
      "metadata": {
        "id": "W6SK3xC_xBUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates(df):\n",
        "    image_hashes = {}\n",
        "    duplicates = []\n",
        "\n",
        "    for img_path in df['image_path']:\n",
        "        try:\n",
        "            with Image.open(img_path) as img:\n",
        "                img_hash = hash(img.tobytes())  # Hash the image content\n",
        "                if img_hash in image_hashes:\n",
        "                    duplicates.append(img_path)  # Save duplicate paths\n",
        "                else:\n",
        "                    image_hashes[img_hash] = img_path\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {img_path}\")\n",
        "        except UnidentifiedImageError:\n",
        "            print(f\"Corrupted image found: {img_path}\")\n",
        "\n",
        "    # Drop duplicates from the DataFrame\n",
        "    df = df[~df['image_path'].isin(duplicates)]\n",
        "    print(f\"Removed {len(duplicates)} duplicates.\")\n",
        "    return df\n",
        "\n",
        "# Remove duplicates from train, val, and test sets\n",
        "train_df = remove_duplicates(train_df)\n",
        "val_df = remove_duplicates(val_df)\n",
        "test_df = remove_duplicates(test_df)"
      ],
      "metadata": {
        "id": "K-UIdFLwxCEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.10 Display Sample Batch<a id='4.10_Batch'></a>"
      ],
      "metadata": {
        "id": "YKzUIOphxFPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(generator):\n",
        "    # Get the first batch of images\n",
        "    images, labels = next(generator)\n",
        "\n",
        "    # Print the shape of the batch to confirm data is loaded\n",
        "    print(f\"Batch shape (images): {images.shape}\")\n",
        "    print(f\"Batch shape (labels): {labels.shape}\")\n",
        "\n",
        "    # Check if images array is not empty\n",
        "    if images.shape[0] > 0:\n",
        "        fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
        "        for i in range(min(5, images.shape[0])):  # Ensure we don't go out of bounds\n",
        "            img = images[i]\n",
        "            axes[i].imshow(img)\n",
        "            axes[i].axis('off')\n",
        "            axes[i].set_title(f'Label: {labels[i]}')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No images in batch to display\")\n",
        "\n",
        "# Show a batch of training images\n",
        "show_images(train_generator)"
      ],
      "metadata": {
        "id": "XkOnEuIDxGZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.11 Save Processed Data<a id='4.11_Save'></a>"
      ],
      "metadata": {
        "id": "4KJuv3N2xJnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save relevant configuration parameters for recreating the generator\n",
        "generator_config = {\n",
        "    'batch_size': train_generator.batch_size,\n",
        "    'class_mode': train_generator.class_mode,\n",
        "    'target_size': train_generator.target_size,\n",
        "    'shuffle': train_generator.shuffle,\n",
        "    'seed': train_generator.seed,\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('train_generator_config.json', 'w') as f:\n",
        "    json.dump(generator_config, f)\n",
        "\n",
        "print(\"Generator configuration saved!\")"
      ],
      "metadata": {
        "id": "uzio-GYixKhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.11 Summary<a id='4.10_Summary'></a>"
      ],
      "metadata": {
        "id": "OFIbsEqoxQvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we preprocessed the chest X-ray dataset for pneumonia detection using TensorFlow/Keras. We loaded the data, applied transformations such as resizing, rescaling, and data augmentation to enhance model generalization. We also removed duplicate images to avoid skewing the training process. After splitting the data into training, validation, and test sets, we created ImageDataGenerator objects to efficiently feed batches during model training. Finally, we saved the preprocessed generators for future use."
      ],
      "metadata": {
        "id": "n76I_P7xxSoo"
      }
    }
  ]
}